{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-overview",
   "metadata": {},
   "source": [
    "# Stock Price Movement Prediction\n",
    "## Objective\n",
    "Predict whether the closing price of an S&P 500 stock will increase or decrease on the next trading day based on the following features:\n",
    "- Opening Price\n",
    "- Highest Price\n",
    "- Lowest Price\n",
    "- Adjusted Close Price\n",
    "- Trading Volume\n",
    "\n",
    "## Data Description\n",
    "- **Date Range**: 2010 to end of 2016\n",
    "- **Companies**: 501 S&P 500 companies\n",
    "- **Data Points**: 851,264 entries\n",
    "- **Adjustments**: 140 stock splits adjusted in `prices-split-adjusted.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importing-libraries",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "Import all essential libraries required for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "# Model Explainability\n",
    "import shap\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot aesthetics\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-data",
   "metadata": {},
   "source": [
    "## Loading and Inspecting the Data\n",
    "Load the dataset, inspect the first few rows, check for missing values, and understand the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('prices-split-adjusted.csv')\n",
    "\n",
    "# Display the first five rows\n",
    "print(\"First five rows of the dataset:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types and non-null counts\n",
    "print(\"\\nData Information:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique values\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Handle missing values, create the target variable, and perform necessary feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target-variable",
   "metadata": {},
   "source": [
    "### Defining the Target Variable\n",
    "Create a binary target variable indicating whether the closing price increases (1) or decreases (0) the next trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by symbol and date to ensure correct ordering\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by=['symbol', 'date'])\n",
    "\n",
    "# Create the target variable: 1 if next day's close > today's close, else 0\n",
    "df['target'] = df.groupby('symbol')['close'].shift(-1) > df['close']\n",
    "df['target'] = df['target'].astype(int)\n",
    "\n",
    "# Drop the last day for each symbol as it doesn't have a target\n",
    "df = df.dropna(subset=['target'])\n",
    "\n",
    "print(\"\\nSample data with target variable:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handle-missing-values",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "Impute or remove missing values as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impute-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Impute missing values for numerical columns using median\n",
    "numerical_cols = ['open', 'close', 'low', 'high', 'volume']\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nMissing Values After Imputation:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create additional features that may help in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily return as a feature\n",
    "df['daily_return'] = (df['close'] - df['open']) / df['open']\n",
    "\n",
    "# Create volatility feature\n",
    "df['volatility'] = (df['high'] - df['low']) / df['open']\n",
    "\n",
    "# Drop any remaining missing values just in case\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"\\nData with new features:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exploratory-data-analysis",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Understand the distribution of features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target-distribution",
   "metadata": {},
   "source": [
    "### Distribution of the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='target', data=df, palette='Set2')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Price Increase (1) or Decrease (0)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print the class balance\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-distributions",
   "metadata": {},
   "source": [
    "### Feature Distributions\n",
    "Visualize the distribution of key numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for plotting\n",
    "features = ['open', 'close', 'low', 'high', 'volume', 'daily_return', 'volatility']\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[feature], kde=True, bins=50)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correlation-analysis",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "Examine the correlation between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df[['open', 'close', 'low', 'high', 'volume', 'daily_return', 'volatility', 'target']].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-preparation",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Encode categorical variables, split the data into training and testing sets, and scale the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding-categorical",
   "metadata": {},
   "source": [
    "### Encoding Categorical Variables\n",
    "Encode the `symbol` column using One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encode-categorical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = ['open', 'close', 'low', 'high', 'volume', 'daily_return', 'volatility', 'symbol']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['symbol']\n",
    "\n",
    "if categorical_cols:\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore')\n",
    "    encoded_features = encoder.fit_transform(X[categorical_cols])\n",
    "    encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Create DataFrame for encoded features\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoded_cols, index=X.index)\n",
    "    \n",
    "    # Concatenate with original features\n",
    "    X = pd.concat([X.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "print(\"\\nFeatures after encoding categorical variables:\")\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-test-split",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Testing Sets\n",
    "Split the data based on chronological order to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by date\n",
    "X = X.sort_values(by='date')\n",
    "y = y.sort_index()\n",
    "\n",
    "# Define the split point (e.g., last 20% as test)\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling-features",
   "metadata": {},
   "source": [
    "### Scaling Features\n",
    "Scale numerical features to ensure they contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scale-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns (excluding the encoded symbols)\n",
    "numerical_cols = ['open', 'close', 'low', 'high', 'volume', 'daily_return', 'volatility']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform both training and testing data\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## Model Training and Hyperparameter Tuning\n",
    "Train multiple classification models and perform hyperparameter tuning to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]}\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train and tune models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    if name in param_grids:\n",
    "        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='f1', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_models[name] = grid.best_estimator_\n",
    "        print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        best_models[name] = model\n",
    "        print(f\"{name} trained without hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-evaluation",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the performance of each model using appropriate classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store evaluation metrics\n",
    "evaluation_metrics = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC'])\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    \n",
    "    evaluation_metrics = evaluation_metrics.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }, ignore_index=True)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics on Test Set:\")\n",
    "display(evaluation_metrics)\n",
    "\n",
    "# Plot evaluation metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "evaluation_metrics_melted = evaluation_metrics.melt(id_vars='Model', value_vars=metrics, var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='Model', data=evaluation_metrics_melted)\n",
    "plt.title('Model Comparison based on Evaluation Metrics')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Visualize the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for each model\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification-report",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "Detailed classification report for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for each model\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-importance",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Identify the most important features for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Random Forest\n",
    "if 'Random Forest' in best_models:\n",
    "    rf = best_models['Random Forest']\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "    importances = importances.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=importances.values, y=importances.index, palette='viridis')\n",
    "    plt.title('Top 10 Feature Importances - Random Forest')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "# Feature Importance for Logistic Regression\n",
    "if 'Logistic Regression' in best_models:\n",
    "    lr = best_models['Logistic Regression']\n",
    "    coefficients = pd.Series(lr.coef_[0], index=X_train.columns)\n",
    "    coefficients = coefficients.abs().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=coefficients.values, y=coefficients.index, palette='magma')\n",
    "    plt.title('Top 10 Feature Coefficients - Logistic Regression')\n",
    "    plt.xlabel('Coefficient Magnitude')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-explainability",
   "metadata": {},
   "source": [
    "## Model Explainability\n",
    "Use SHAP values to interpret the contribution of each feature to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shap-explain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on F1 Score\n",
    "best_model_name = evaluation_metrics.sort_values(by='F1 Score', ascending=False).iloc[0]['Model']\n",
    "best_model = best_models[best_model_name]\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(best_model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dependence plot for the top feature\n",
    "top_feature = shap_values.feature_names[0]\n",
    "shap.dependence_plot(top_feature, shap_values.values, X_test, show=False)\n",
    "plt.title(f'SHAP Dependence Plot for {top_feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Summarize the findings, model performance, and potential next steps for improving predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
